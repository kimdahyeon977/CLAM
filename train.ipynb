{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdahyeon977/CLAM/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g3wW7C348r_",
        "outputId": "918a9b28-21c2-49d0-9d6a-7c4225db9afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_-IxIjxI7uD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/CycleGAN-PyTorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EUcxxbLMa_h"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import itertools\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import models  # models.py 코드를 임포트 해주고 있음\n",
        "import utils\n",
        "from eval import evaluate\n",
        "import sys\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vknN2MLlpCf8"
      },
      "outputs": [],
      "source": [
        "def initWeights(m):\n",
        "    \"\"\"\n",
        "    어떤 모델 m의 가중치를 초기화 해주는 함수\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:  # 합성곱 계층이 존재하면\n",
        "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)  # m의 합성곱 계층들의 가중치를 평균 0.0 표준편차 0.02인 정규 분포로 초기화\n",
        "    elif classname.find('BatchNorm2d') != -1:  # BatchNorm2d가 존재하면\n",
        "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)  # 초기화를 평균 1.0 표준편차 0.02인 정규 분포로 초기화\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)  # 편향 값들은 0.0의 상수로 초기화\n",
        "\n",
        "class LR_sched():\n",
        "    def __init__(self, numEpochs, decayEpoch):\n",
        "        assert ((numEpochs - decayEpoch) > 0), \"ohh no, decay > number epochs\"\n",
        "        self.numEpochs = numEpochs\n",
        "        self.decayEpoch = decayEpoch\n",
        "    def step(self, currentEpoch):\n",
        "        return 1.0 - max(0, currentEpoch - self.decayEpoch)/(self.numEpochs - self.decayEpoch)\n",
        "#---------\n",
        "\n",
        "#----------\n",
        "class ImageBuffer():\n",
        "    \"\"\"\n",
        "    이미지를 버퍼라고 하는 변수에 넣어뒀다가, 새로운 데이터가 들어오면 기존에 있던거는 출력하고, 새로운 데이터를 저장하는 코드\n",
        "    \"\"\"\n",
        "    def __init__(self, size=50):\n",
        "        self.size = size\n",
        "        self.bufferSize = 0\n",
        "        self.buffer = []\n",
        "    def pushPop(self, data):\n",
        "        if self.size == 0:\n",
        "            return data\n",
        "        returnData = []\n",
        "        for element in data:\n",
        "            element = torch.unsqueeze(element.data, 0)  # 데이터에 차원을 하나 늘려주는 코드\n",
        "            if self.bufferSize < self.size:\n",
        "                self.bufferSize +=  1\n",
        "                self.buffer.append(element)\n",
        "                returnData.append(element)\n",
        "            else:\n",
        "                p = random.uniform(0, 1)\n",
        "                if p > 0.5:\n",
        "                    random_id = random.randint(0, self.size - 1)\n",
        "                    tmp = self.buffer[random_id].clone()\n",
        "                    returnData.append(tmp)\n",
        "                    self.buffer[random_id] = element\n",
        "                else:\n",
        "                    returnData.append(element)\n",
        "        return torch.cat(returnData, 0)\n",
        "#----------\n",
        "class LossLogger():\n",
        "    def __init__(self, numEpochs, numBatches):\n",
        "        self.numEpochs =numEpochs\n",
        "        self.numBatches = numBatches\n",
        "        self.losses = {}\n",
        "        self.timeStart = time.time()\n",
        "        self.timeBatchAvg = 0\n",
        "\n",
        "    def log(self, currentEpoch, currentBatch, losses):\n",
        "        sys.stdout.write('\\rEpoch %03d/%03d [%04d/%04d] | ' % (currentEpoch, self.numEpochs, currentBatch, self.numBatches))\n",
        "        for lossName in losses:\n",
        "            if lossName not in self.losses:\n",
        "                self.losses[lossName] = []\n",
        "                self.losses[lossName].append(losses[lossName].item())\n",
        "            else:\n",
        "                if len(self.losses[lossName]) < currentEpoch:\n",
        "                    self.losses[lossName].append(losses[lossName].item())\n",
        "                else:\n",
        "                    self.losses[lossName][-1] += losses[lossName].item()\n",
        "            sys.stdout.write('%s: %.4f | ' % (lossName, self.losses[lossName][-1]/currentBatch))\n",
        "            if currentBatch % self.numBatches == 0 :\n",
        "                self.losses[lossName][-1] *= 1./currentBatch\n",
        "\n",
        "        batchesDone =  (currentEpoch-1)*self.numBatches + currentBatch\n",
        "        self.timeBatchAvg = (time.time() - self.timeStart)/float(batchesDone)\n",
        "        batchesLeft = self.numEpochs*self.numBatches - batchesDone\n",
        "        sys.stdout.write('ETA: %s' % (datetime.timedelta(seconds=batchesLeft*self.timeBatchAvg)))\n",
        "\n",
        "        if currentBatch % self.numBatches == 0 :\n",
        "            sys.stdout.write('\\n')\n",
        "\n",
        "    def plot(self):\n",
        "        for lossName in self.losses:\n",
        "            plt.figure()\n",
        "            plt.plot(range(len(self.losses[lossName])),self.losses[lossName])\n",
        "            plt.title(lossName)\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.savefig('output/'+lossName+'.png')\n",
        "\n",
        "    def save(self):\n",
        "        df = pd.DataFrame.from_dict(self.losses)\n",
        "        df.to_csv(\"output/losses.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgzdbhJtUazN"
      },
      "outputs": [],
      "source": [
        "def initWeights(m):\n",
        "    \"\"\"\n",
        "    어떤 모델 m의 가중치를 초기화 해주는 함수\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:  # 합성곱 계층이 존재하면\n",
        "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)  # m의 합성곱 계층들의 가중치를 평균 0.0 표준편차 0.02인 정규 분포로 초기화\n",
        "    elif classname.find('BatchNorm2d') != -1:  # BatchNorm2d가 존재하면\n",
        "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)  # 초기화를 평균 1.0 표준편차 0.02인 정규 분포로 초기화\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)  # 편향 값들은 0.0의 상수로 초기화\n",
        "\n",
        "class LR_sched():\n",
        "    def __init__(self, numEpochs, decayEpoch):\n",
        "        assert ((numEpochs - decayEpoch) > 0), \"ohh no, decay > number epochs\"\n",
        "        self.numEpochs = numEpochs\n",
        "        self.decayEpoch = decayEpoch\n",
        "    def step(self, currentEpoch):\n",
        "        return 1.0 - max(0, currentEpoch - self.decayEpoch)/(self.numEpochs - self.decayEpoch)\n",
        "#---------\n",
        "\n",
        "#----------\n",
        "class ImageBuffer():\n",
        "    \"\"\"\n",
        "    이미지를 버퍼라고 하는 변수에 넣어뒀다가, 새로운 데이터가 들어오면 기존에 있던거는 출력하고, 새로운 데이터를 저장하는 코드\n",
        "    \"\"\"\n",
        "    def __init__(self, size=50):\n",
        "        self.size = size\n",
        "        self.bufferSize = 0\n",
        "        self.buffer = []\n",
        "    def pushPop(self, data):\n",
        "        if self.size == 0:\n",
        "            return data\n",
        "        returnData = []\n",
        "        for element in data:\n",
        "            element = torch.unsqueeze(element.data, 0)  # 데이터에 차원을 하나 늘려주는 코드\n",
        "            if self.bufferSize < self.size:\n",
        "                self.bufferSize +=  1\n",
        "                self.buffer.append(element)\n",
        "                returnData.append(element)\n",
        "            else:\n",
        "                p = random.uniform(0, 1)\n",
        "                if p > 0.5:\n",
        "                    random_id = random.randint(0, self.size - 1)\n",
        "                    tmp = self.buffer[random_id].clone()\n",
        "                    returnData.append(tmp)\n",
        "                    self.buffer[random_id] = element\n",
        "                else:\n",
        "                    returnData.append(element)\n",
        "        return torch.cat(returnData, 0)\n",
        "#----------\n",
        "class LossLogger():\n",
        "    def __init__(self, numEpochs, numBatches):\n",
        "        self.numEpochs =numEpochs\n",
        "        self.numBatches = numBatches\n",
        "        self.losses = {}\n",
        "        self.timeStart = time.time()\n",
        "        self.timeBatchAvg = 0\n",
        "\n",
        "    def log(self, currentEpoch, currentBatch, losses):\n",
        "        sys.stdout.write('\\rEpoch %03d/%03d [%04d/%04d] | ' % (currentEpoch, self.numEpochs, currentBatch, self.numBatches))\n",
        "        for lossName in losses:\n",
        "            if lossName not in self.losses:\n",
        "                self.losses[lossName] = []\n",
        "                self.losses[lossName].append(losses[lossName].item())\n",
        "            else:\n",
        "                if len(self.losses[lossName]) < currentEpoch:\n",
        "                    self.losses[lossName].append(losses[lossName].item())\n",
        "                else:\n",
        "                    self.losses[lossName][-1] += losses[lossName].item()\n",
        "            sys.stdout.write('%s: %.4f | ' % (lossName, self.losses[lossName][-1]/currentBatch))\n",
        "            if currentBatch % self.numBatches == 0 :\n",
        "                self.losses[lossName][-1] *= 1./currentBatch\n",
        "\n",
        "        batchesDone =  (currentEpoch-1)*self.numBatches + currentBatch\n",
        "        self.timeBatchAvg = (time.time() - self.timeStart)/float(batchesDone)\n",
        "        batchesLeft = self.numEpochs*self.numBatches - batchesDone\n",
        "        sys.stdout.write('ETA: %s' % (datetime.timedelta(seconds=batchesLeft*self.timeBatchAvg)))\n",
        "\n",
        "        if currentBatch % self.numBatches == 0 :\n",
        "            sys.stdout.write('\\n')\n",
        "\n",
        "    def plot(self):\n",
        "        for lossName in self.losses:\n",
        "            plt.figure()\n",
        "            plt.plot(range(len(self.losses[lossName])),self.losses[lossName])\n",
        "            plt.title(lossName)\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.savefig('output/'+lossName+'.png')\n",
        "\n",
        "    def save(self):\n",
        "        df = pd.DataFrame.from_dict(self.losses)\n",
        "        df.to_csv(\"output/losses.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TifTIr1dUcWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360b9fbd-1eb2-4e8c-af09-4f3ccd3407e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--sample_ratio'], dest='sample_ratio', nargs=None, const=None, default=1, type=<class 'float'>, choices=None, required=False, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser()  # 하이퍼파라미터 설정\n",
        "# 사용자로부터 하이퍼파라미터를 입력받는 역할\n",
        "# add_argument 하나하나가 사용자에게 입력받을 하이퍼파라미터를 정의하는 부분\n",
        "# parser.add_argument('--epoch', type=int, default=0, help='starting epoch')\n",
        "parser.add_argument('--numEpochs', type=int, default=200, help='number of training epochs')\n",
        "parser.add_argument('--batchSize', type=int, default=1, help='batch size')\n",
        "parser.add_argument('--dataroot', type=str, default='../datasets_npy/train/', help='directory of the dataset')\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate')\n",
        "parser.add_argument('--decayEpoch', type=int, default=100, help='epoch to start linearly decaying the learning rate to 0')\n",
        "parser.add_argument('--lambdaCyc_x', type=float, default=10.0, help='lambda for cycle loss (x -> y -> x)')\n",
        "parser.add_argument('--lambdaCyc_y', type=float, default=10.0, help='lambda for cycle loss (y -> x -> y)')\n",
        "parser.add_argument('--lambdaIdentity', type=float, default=5.0, help='lambda for identity loss')\n",
        "parser.add_argument('--size', type=int, default=512, help='size of squared img to use (resize and crop)')\n",
        "parser.add_argument('--input_nc', type=int, default=1, help='number of channels of input data')\n",
        "parser.add_argument('--output_nc', type=int, default=1, help='number of channels of output data')\n",
        "parser.add_argument('--cuda', action='store_true', default=True, help='use GPU computation')\n",
        "parser.add_argument('--n_cpu', type=int, default=12, help='number of cpu threads to use during batch generation')\n",
        "parser.add_argument('--saveEpochFrq', type=int, default=20, help='frequency of saving checkpoints at the end of epochs') # 몇에폭마다 output 저장할것인지\n",
        "parser.add_argument('--manualSeed', action='store_true', help='use manual seed')\n",
        "parser.add_argument('--seedNum', type=int, default=6, help='seed')\n",
        "parser.add_argument('--imageBuffer', action='store_true', help='use an image buffer')\n",
        "parser.add_argument('--evaluation_on', type=str, default=\"test\", help=\"data to use in evaluation\")\n",
        "\n",
        "parser.add_argument('--numEpochs_total', type=int, default=None, help='number of total training epochs')\n",
        "parser.add_argument('--continue_epoch', type=int, default=0, help=\"continue training from epoch #\")\n",
        "parser.add_argument('--genG', type=str, default='output/netG.pth', help='generator checkpoint file x->y')\n",
        "parser.add_argument('--genF', type=str, default='output/netF.pth', help='generator checkpoint file y->x')\n",
        "parser.add_argument('--netD_x', type=str, default='output/netD_x.pth', help='Discriminator checkpoint file D_x')\n",
        "parser.add_argument('--netD_y', type=str, default='output/netD_y.pth', help='Discriminator checkpoint file D_y')\n",
        "parser.add_argument('--discratio',type=int, default=5)\n",
        "parser.add_argument('--sample_ratio',type=float, default=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4drBV6uWdCY"
      },
      "outputs": [],
      "source": [
        "# opt = parser.parse_args(['--numEpochs', '40, '--batchSize', '4', '--decayEpoch', '100', '--lr', '0.0002', '--evaluation_on', 'train', '--numEpochs_total', '200',\n",
        "#                          '--lambdaCyc_x', '25', '--lambdaCyc_y', '25', '--lambdaIdentity', '0.5', '--continue_epoch','21','--discratio','7'])  # 사용자에게 하이퍼파라미터 입력받아서 opt에 저장을 한 것 # EX : continue_epoch 20 num epoch 40\n",
        "# print(opt)\n",
        "# #-1028-2048 바꿔 보는것을 추천 색이이상 3d 슬라이스에  cbct나 spct 범위 같은지 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0XKxY2CW7E"
      },
      "source": [
        "opt = parser.parse_args(['--numEpochs', '120', '--batchSize', '4', '--decayEpoch', '100', '--lr', '0.0002', '--evaluation_on', 'train', '--numEpochs_total', '200',\n",
        "                         '--lambdaCyc_x', '25', '--lambdaCyc_y', '25', '--lambdaIdentity', '0.5', '--continue_epoch','85','--discratio','7', '--sample_ratio','0.5'])  # 사용자에게 하이퍼파라미터 입력받아서 opt에 저장을 한 것 # EX : continue_epoch 20 num epoch 40\n",
        "print(opt)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juEYUEjcUXEu"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiVWOY3BaDpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fee1f35-b73c-40cf-9266-7e613b90a241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(numEpochs=120, batchSize=4, dataroot='./datasets_npy', lr=0.0002, decayEpoch=51, lambdaCyc_x=15.0, lambdaCyc_y=15.0, lambdaIdentity=2.0, size=512, input_nc=1, output_nc=1, cuda=True, n_cpu=12, saveEpochFrq=20, manualSeed=False, seedNum=6, imageBuffer=False, evaluation_on='train', numEpochs_total=200, continue_epoch=112, genG='output/netG.pth', genF='output/netF.pth', netD_x='output/netD_x.pth', netD_y='output/netD_y.pth', discratio=9, sample_ratio=1.0)\n"
          ]
        }
      ],
      "source": [
        "opt = parser.parse_args(['--numEpochs', '120', '--batchSize', '4', '--decayEpoch', '51', '--lr', '0.0002', '--evaluation_on', 'train', '--numEpochs_total', '200', #evalution_on test 로 하면 test 평\n",
        "                         '--lambdaCyc_x', '15', '--lambdaCyc_y', '15', '--lambdaIdentity', '2', '--continue_epoch','112','--discratio','9', '--sample_ratio','1',\"--dataroot\", \"./datasets_npy\",'--size','512'])  # 사용자에게 하이퍼파라미터 입력받아서 opt에 저장을 한 것 # EX : continue_epoch 20 num epoch 40\n",
        "print(opt)\n",
        "#decayEpoch - 특정 에폭까지는 러닝레이트 쓰다가 decay부터는 학습 낮춰준다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuXWM_O5Upw8"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5o9Fw3AUrFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7c825f-a48d-4a2f-f066-bfca83c5ac74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 재개\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "<ipython-input-13-17f683b3eb72>:65: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_x = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
            "환자별 이미지 적재 중: 100%|██████████| 17/17 [00:17<00:00,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "환자 17명 | CBCT 1022장 | SIM 855장 확인\n",
            "sample ratio: 1.0 | CBCT 855장 | SIM 855장 적재\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "if opt.numEpochs_total is None:\n",
        "    opt.numEpochs_total = opt.numEpochs\n",
        "\n",
        "if opt.manualSeed:\n",
        "    torch.cuda.manual_seed(opt.seedNum)\n",
        "    torch.cuda.manual_seed_all(opt.seedNum)\n",
        "\n",
        "# 모델을 선언해주는 코드\n",
        "G = models.Generator(opt.input_nc, opt.output_nc) #generator x->y\n",
        "F = models.Generator(opt.output_nc, opt.input_nc) #generator y->x\n",
        "D_x = models.Discriminator(opt.input_nc) #discriminator X\n",
        "D_y = models.Discriminator(opt.input_nc) #discriminator Y\n",
        "\n",
        "if opt.cuda:  # CUDA를 사용할 것이면?\n",
        "    G.cuda()  # 각 모델을 GPU로 보내주는 코드\n",
        "    F.cuda()\n",
        "    D_x.cuda()\n",
        "    D_y.cuda()\n",
        "\n",
        "if opt.continue_epoch == 0:  # 처음부터 학습할 경우\n",
        "    G.apply(initWeights)\n",
        "    F.apply(initWeights)\n",
        "    D_x.apply(initWeights)\n",
        "    D_y.apply(initWeights)\n",
        "else:  # 학습을 이어서 진행할 경우\n",
        "    print(\"학습 재개\")\n",
        "    G.load_state_dict(torch.load(opt.genG))\n",
        "    F.load_state_dict(torch.load(opt.genF))\n",
        "    D_x.load_state_dict(torch.load(opt.netD_x))\n",
        "    D_y.load_state_dict(torch.load(opt.netD_y))\n",
        "\n",
        "\n",
        "# loss를 선언하는 부분\n",
        "criterionGAN = torch.nn.MSELoss()  # 파이토치에 기본으로 탑재된 Loss들을 사용\n",
        "criterionCycle = torch.nn.L1Loss()\n",
        "criterionIdentity = torch.nn.L1Loss()\n",
        "\n",
        "#   optim\n",
        "optimizer_Generators = torch.optim.AdamW(itertools.chain(G.parameters(), F.parameters()),\n",
        "                                lr=opt.lr, betas=(0.5, 0.999))\n",
        "optimizer_D_x = torch.optim.AdamW(D_x.parameters(), lr=opt.lr*0.1, betas=(0.5, 0.999))    #discrminator lr generator 의 1/10 으로 학습\n",
        "optimizer_D_y = torch.optim.AdamW(D_y.parameters(), lr=opt.lr*0.1, betas=(0.5, 0.999))\n",
        "\n",
        "# 러닝레이트 스케쥴링\n",
        "# lrScheduler_Generators = torch.optim.lr_scheduler.LambdaLR(optimizer_Generators, lr_lambda=LR_sched(opt.numEpochs_total, opt.decayEpoch).step)\n",
        "# lrScheduler_D_x = torch.optim.lr_scheduler.LambdaLR(optimizer_D_x, lr_lambda=LR_sched(opt.numEpochs_total, opt.decayEpoch).step)\n",
        "# lrScheduler_D_y = torch.optim.lr_scheduler.LambdaLR(optimizer_D_y, lr_lambda=LR_sched(opt.numEpochs_total, opt.decayEpoch).step)\n",
        "\n",
        "### --- ADDED:\n",
        "import torch.optim as optim\n",
        "\n",
        "lrScheduler_Generators = optim.lr_scheduler.CosineAnnealingLR(optimizer_Generators, T_max=opt.numEpochs_total)\n",
        "lrScheduler_D_x = optim.lr_scheduler.CosineAnnealingLR(optimizer_D_x, T_max=opt.numEpochs_total)\n",
        "lrScheduler_D_y = optim.lr_scheduler.CosineAnnealingLR(optimizer_D_y, T_max=opt.numEpochs_total)\n",
        "### --- end of ADDED\n",
        "\n",
        "# 학습 진행한 만큼 lr Scheduler step 반영\n",
        "for epoch in range(opt.continue_epoch):\n",
        "    lrScheduler_Generators.step()\n",
        "    lrScheduler_D_x.step()\n",
        "    lrScheduler_D_y.step()\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
        "\n",
        "input_x = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
        "input_y = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
        "targetReal = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)\n",
        "targetFake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)\n",
        "\n",
        "if opt.imageBuffer:\n",
        "    bufferFake_x = ImageBuffer()\n",
        "    bufferFake_y = ImageBuffer()\n",
        "\n",
        "transformList = [\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomCrop(opt.size//2),\n",
        "    transforms.Resize(opt.size),\n",
        "    # transforms.RandomHorizontalFlip(),   #vertical and horizental 한개만\n",
        "    #transforms.RandomVerticalFlip(),                                                     # 파이토치 텐서로 만들어주고 값 범위를 0~1로 조정          #크롭하면 리사이즈 주석해줘야한다\n",
        "    ### --- ADDED:\n",
        "    # transforms.RandomCrop(opt.size),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    ### --- end of ADDED\n",
        "    transforms.Normalize((0.5), (0.5)),  # (x-0.5)/0.5 해서 범위를 -1~1로 조정\n",
        "    #transforms.Resize(opt.size)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset = DataLoader(utils.LoadDataset('../datasets_npy', transformList=transformList, sample_ratio=opt.sample_ratio),\n",
        "                        batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 학습 경과를 기록하는 코드\n",
        "logger = LossLogger(opt.numEpochs, len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH8AxcGldlVV"
      },
      "outputs": [],
      "source": [
        "EXP_NAME=\"Quantification\"\n",
        "try:\n",
        "  log_csv = pd.read_csv(f\"{EXP_NAME}.csv\")\n",
        "except:\n",
        "  log_csv = pd.DataFrame(columns=[\"epoch\",\"Loss_Gen_GAN\",\"Loss_Gen_Cycle\",\"Loss_Gen_Identity\",\"Loss_Gen\",\"Loss_Disc\",\"MAE\",\"RMSE\",\"PSNR\",\"SSIM\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NWRgwwkbsRv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a3c02b8c-5013-4b69-fb54-6f25b6e467d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [epoch, Loss_Gen_GAN, Loss_Gen_Cycle, Loss_Gen_Identity, Loss_Gen, Loss_Disc, MAE, RMSE, PSNR, SSIM]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb9f36b7-5db7-4e58-9e53-c0ccd822e091\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>Loss_Gen_GAN</th>\n",
              "      <th>Loss_Gen_Cycle</th>\n",
              "      <th>Loss_Gen_Identity</th>\n",
              "      <th>Loss_Gen</th>\n",
              "      <th>Loss_Disc</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>PSNR</th>\n",
              "      <th>SSIM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb9f36b7-5db7-4e58-9e53-c0ccd822e091')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb9f36b7-5db7-4e58-9e53-c0ccd822e091 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb9f36b7-5db7-4e58-9e53-c0ccd822e091');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_20ebb48b-a3cc-4a9c-a7e3-80a3ccbcb1f1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('log_csv')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_20ebb48b-a3cc-4a9c-a7e3-80a3ccbcb1f1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('log_csv');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "log_csv",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "log_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szZ1WvCVzwVA"
      },
      "outputs": [],
      "source": [
        "# MAE evaluation = 동일한 slice , ROI 평가 ,   학습 = 중간지점에서 0.2 .0.3  몇장 ,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "6qUxwL8tWjEV",
        "outputId": "c3e6f5b3-57da-4f5a-aa34-f349ce2443bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/213 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/213 [00:15<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 64.12 MiB is free. Process 5184 has 14.68 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 127.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f17351d60d69>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#cycle loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mrecovered_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# F(G(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mrecovered_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# G(F(y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlossCyc_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterionCycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecovered_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentBatch_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# | F(G(X)) - x |\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/CycleGAN-PyTorch/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/CycleGAN-PyTorch/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_no_batch_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_instance_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36m_apply_instance_norm\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_instance_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         return F.instance_norm(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_input_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2875\u001b[0m         \u001b[0m_verify_spatial_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2876\u001b[0;31m     return torch.instance_norm(\n\u001b[0m\u001b[1;32m   2877\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 64.12 MiB is free. Process 5184 has 14.68 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 127.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "#   --------------------- TRAIN\n",
        "for epoch in range(opt.continue_epoch, opt.numEpochs+1):\n",
        "    losses = defaultdict(list) #딕셔너리를 만들건데 , 이 딕셔너리의 기본값은 비어있는 리스트로 설정한다\n",
        "    for i, batch in enumerate(tqdm(dataset)):  # 데이터셋에서 batch_size 개의 데이터 불러오기\n",
        "        currentBatch_x = Variable(input_x.copy_(batch['x']))  # 데이터를 불러와서, Variable로 감싸주고\n",
        "        currentBatch_y = Variable(input_y.copy_(batch['y']))\n",
        "\n",
        "        # import pdb;pdb.set_trace()\n",
        "\n",
        "        # 데이터를 모델에 입력\n",
        "        fake_y = G(currentBatch_x) #G(x)\n",
        "        fake_x = F(currentBatch_y) #F(y)\n",
        "\n",
        "        #-------- Generators loss\n",
        "        optimizer_Generators.zero_grad()  # 옵티마이저를 초기화\n",
        "\n",
        "        #lsgan loss\n",
        "        lossGAN_G = criterionGAN(D_y(fake_y).squeeze(), targetReal.squeeze())  # (D_y(G(x)) - 1)^2\n",
        "        lossGAN_F = criterionGAN(D_x(fake_x).squeeze(), targetReal.squeeze())  # (D_x(F(y)) - 1)^2\n",
        "\n",
        "        #cycle loss\n",
        "        recovered_x = F(fake_y) # F(G(x))\n",
        "        recovered_y = G(fake_x) # G(F(y))\n",
        "        lossCyc_x = criterionCycle(recovered_x, currentBatch_x) # | F(G(X)) - x |\n",
        "        lossCyc_y = criterionCycle(recovered_y, currentBatch_y) # | G(F(y)) - y |\n",
        "        lossCyc = lossCyc_x*opt.lambdaCyc_x + lossCyc_y*opt.lambdaCyc_y\n",
        "\n",
        "        #identity loss\n",
        "        lossId_x = criterionIdentity(F(currentBatch_x), currentBatch_x) # | F(x) - x |\n",
        "        lossId_y = criterionIdentity(G(currentBatch_y), currentBatch_y) # | G(y) - y |\n",
        "        lossId = (lossId_x + lossId_y)*opt.lambdaIdentity\n",
        "\n",
        "        #total generators loss\n",
        "        loss_Generators = lossGAN_G + lossGAN_F + lossCyc + lossId\n",
        "        loss_Generators.backward()\n",
        "\n",
        "        optimizer_Generators.step()\n",
        "\n",
        "        #-------- Discriminator loss\n",
        "        optimizer_D_x.zero_grad()  # optimizer 초기화 해주고\n",
        "        # Loss 계산\n",
        "        if opt.imageBuffer:\n",
        "            lossGAN_D_x = (criterionGAN(D_x(currentBatch_x).squeeze(), targetReal.squeeze()) + criterionGAN(D_x(bufferFake_x.pushPop(fake_x).detach()).squeeze(), targetFake.squeeze()))*0.5 #(D_x(x)-1)^2 + (D_x(F(y)))^2\n",
        "        else:\n",
        "            lossGAN_D_x = (criterionGAN(D_x(currentBatch_x).squeeze(), targetReal.squeeze()) + criterionGAN(D_x(fake_x.detach()).squeeze(), targetFake.squeeze()))*0.5 #(D_x(x)-1)^2 + (D_x(F(y)))^2\n",
        "        lossGAN_D_x.backward()  # 그래디언트 구하기\n",
        "        if i%opt.discratio==0: #5번학습할때 1번 discr 학습\n",
        "           optimizer_D_x.step()  # update!\n",
        "\n",
        "        optimizer_D_y.zero_grad()  # optimizer 초기화\n",
        "        if opt.imageBuffer:\n",
        "            lossGAN_D_y = (criterionGAN(D_y(currentBatch_y).squeeze(), targetReal.squeeze()) + criterionGAN(D_y(bufferFake_y.pushPop(fake_y).detach()).squeeze(), targetFake.squeeze()))*0.5 #(D_y(y)-1)^2 + (D_y(G(x)))^2\n",
        "        else:\n",
        "            lossGAN_D_y = (criterionGAN(D_y(currentBatch_y).squeeze(), targetReal.squeeze()) + criterionGAN(D_y(fake_y.detach()).squeeze(), targetFake.squeeze()))*0.5 #(D_y(y)-1)^2 + (D_y(G(x)))^2\n",
        "        lossGAN_D_y.backward()\n",
        "        if i%opt.discratio==0: #5번학습할때 1번 discr 학습\n",
        "           optimizer_D_y.step() # 똑같음\n",
        "\n",
        "        # 학습 끝\n",
        "\n",
        "        # 로스값들 저장\n",
        "        losses[\"Loss_Gen\"] += [loss_Generators.item()]\n",
        "        losses[\"Loss_Gen_GAN\"] += [lossGAN_G.item() + lossGAN_F.item()]\n",
        "        losses[\"Loss_Gen_Identity\"] += [lossId.item()]\n",
        "        losses[\"Loss_Gen_Cycle\"] += [lossCyc.item()]\n",
        "        losses[\"Loss_Disc\"] += [lossGAN_D_x.item() + lossGAN_D_y.item()]\n",
        "\n",
        "    for k , v in losses.items():  #지금까지 기록된 step별 loss의\n",
        "\n",
        "        losses[k]=np.mean(v)\n",
        "\n",
        "        # losses = {'loss_Gen': loss_Generators.item(),\n",
        "        #           'loss_Gen_GAN': (lossGAN_G + lossGAN_F).item(),\n",
        "        #             'loss_Gen_identity': lossId.item(),\n",
        "        #             'loss_Gen_cycle': (lossCyc).item(),\n",
        "        #             'loss_Disc': (lossGAN_D_x + lossGAN_D_y).item()}\n",
        "        # print(losses)\n",
        "\n",
        "    # eval_results = evaluate(G, opt)\n",
        "    # mae = eval_results[\"mae\"]\n",
        "    # rmse = eval_results[\"rmse\"]\n",
        "    # psnr = eval_results[\"psnr\"]\n",
        "    # ssim = eval_results[\"ssim\"]\n",
        "    print(f\"\"\"Epoch: {epoch+1}/{opt.numEpochs} Loss_Gen_GAN: {losses[\"Loss_Gen_GAN\"]:.4f} Loss_Gen_Cycle: {losses[\"Loss_Gen_Cycle\"]:.4f} Loss_Gen_IDT: {losses[\"Loss_Gen_Identity\"]:.4f} Loss_Gen: {losses[\"Loss_Gen\"]:.4f} Loss_Disc:{losses[\"Loss_Disc\"]:.4f}\"\"\")\n",
        "    # print(f\"MAE: {mae:.4f}\\t RMSE: {rmse:.4f}\\t PSNR: {psnr:.4f}\\t SSIM: {ssim:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    img = ((fake_y.detach().cpu()[0].permute(1, 2, 0).numpy() * 0.5) + 0.5) * 255\n",
        "    img = img.astype(int)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "    # with open(\"./output/log.txt\", \"a\") as f:\n",
        "    #     print(f\"Epoch: {epoch+1}/{opt.numEpochs} Loss_Gen_GAN: {(lossGAN_G + lossGAN_F):.4f} Loss_Gen_Cycle: {lossCyc:.4f} Loss_Gen_IDT: {lossId:.4f} Loss_Gen: {loss_Generators:.4f} Loss_Disc:{(lossGAN_D_x + lossGAN_D_y):.4f}\")\n",
        "    #     print(f\"MAE: {mae}\\t RMSE: {rmse}\\t PSNR: {psnr}\\t SSIM: {ssim}\\t\", file=f)\n",
        "\n",
        "#저장코드\n",
        "    log = {'Loss_Gen': losses[\"Loss_Gen\"],\n",
        "            'Loss_Gen_GAN': losses[\"Loss_Gen_GAN\"],\n",
        "            'Loss_Gen_Identity': losses[\"Loss_Gen_Identity\"],\n",
        "            'Loss_Gen_Cycle': losses[\"Loss_Gen_Cycle\"],\n",
        "            'Loss_Disc': losses[\"Loss_Disc\"],\n",
        "            'epoch': epoch,\n",
        "            # 'MAE': mae,\n",
        "            # 'RMSE': rmse,\n",
        "            # 'PSNR': psnr,\n",
        "            # 'SSIM': ssim\n",
        "           }\n",
        "    log=pd.DataFrame([log])\n",
        "    log_csv =pd.concat([log_csv,log],ignore_index=True)\n",
        "    log_csv.to_csv(f\"{EXP_NAME}.csv\", index=False) #로그 파일로 저장\n",
        "\n",
        "    lrScheduler_Generators.step()\n",
        "    lrScheduler_D_x.step()\n",
        "    lrScheduler_D_y.step()\n",
        "\n",
        "    # Save models\n",
        "    if epoch % opt.saveEpochFrq == 0:  # saveEpochFrq 마다 중간 저장\n",
        "        label = '_ep'+str(epoch)\n",
        "        torch.save(G.state_dict(), 'output/netG'+label+'.pth')\n",
        "        torch.save(F.state_dict(), 'output/netF'+label+'.pth')\n",
        "        torch.save(D_x.state_dict(), 'output/netD_x'+label+'.pth')\n",
        "        torch.save(D_y.state_dict(), 'output/netD_y'+label+'.pth')\n",
        "\n",
        "    torch.save(G.state_dict(), 'output/netG.pth')\n",
        "    torch.save(F.state_dict(), 'output/netF.pth')\n",
        "    torch.save(D_x.state_dict(), 'output/netD_x.pth')\n",
        "    torch.save(D_y.state_dict(), 'output/netD_y.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjt6tS1sW2Um"
      },
      "outputs": [],
      "source": [
        "!python eval.py --batchSize 1 --dataroot ./datasets/cbct2sct/ --size 512 --input_nc 1 --output_nc 1 --cuda --genG output/netG.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px2m4LuJ5hd9"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}